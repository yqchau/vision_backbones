import argparse
import os
import test

import pandas as pd
from hydra import compose, initialize_config_dir
from tqdm import tqdm


def parse_opt():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--initial-benchmark-path",
        type=str,
        default="./results/full_speed_benchmark.csv",
        help="Path to CSV File Generated by benchmark.py",
    )
    parser.add_argument(
        "--model-type",
        type=str,
        default="trt_speed_fp32",
        help="Name of Column to Calculate Accuracy",
    )
    parser.add_argument(
        "--metric-name",
        type=str,
        default="trt_top1_fp32",
        help="Name of Accuracy Column",
    )
    parser.add_argument(
        "--final-benchmark-path",
        type=str,
        default="./benchmark_list.csv",
        help="Final Benchmark File Location",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=1,
        help="batch size",
    )
    parser.add_argument(
        "--fp16",
        action="store_true",
        default=False,
        help="Get inference for FP16 Optimised Model",
    )
    parser.add_argument(
        "--exclude-list",
        type=list,
        default=["resnest50d_1s4x24d"],
        help="list of model to exclude from benchmarking",
    )
    return parser.parse_args()


def gpu_main(opt, config):
    benchmark_df = pd.read_csv(opt.initial_benchmark_path)
    trt_df = benchmark_df[benchmark_df[opt.model_type].notnull()]

    for i, row in tqdm(trt_df.iterrows(), total=trt_df.shape[0]):
        is_excluded = False
        for excluded in opt.exclude_list:
            if row["model"].startswith(excluded):
                is_excluded = True
                break
        if is_excluded:
            continue

        trt_opt = True if opt.model_type.startswith("trt") else False
        config["tester"]["trt_opt"] = trt_opt
        config["tester"]["architecture"] = row["model"]
        config["tester"]["measure_speed"] = False
        config["tester"]["batch_size"] = opt.batch_size
        config["tester"]["fp16_mode"] = opt.fp16

        try:
            accuracy, speed = test.get_metrics(config)
            benchmark_df.at[i, opt.metric_name] = accuracy
        except Exception:
            continue

    benchmark_df.to_csv(opt.final_benchmark_path, index=False)
    return benchmark_df


if __name__ == "__main__":

    # Future reference: Customise TensorRT optimisation (https://github.com/NVIDIA-AI-IOT/torch2trt/issues/568)
    # Future works: Can do for CPU (OpenVINO)
    opt = parse_opt()

    with initialize_config_dir(
        version_base=None,
        config_dir=os.path.join(os.getcwd(), "configuration"),
        job_name="model_selection",
    ):
        config = compose(config_name="configs")
        final = gpu_main(opt, config)
        print(final)
